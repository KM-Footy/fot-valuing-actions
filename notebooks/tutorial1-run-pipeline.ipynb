{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Download data, value actions and rate players\n",
    "\n",
    "This tutorial demonstrates how to value on-the-ball actions of football players with the open-source [VAEP framework](https://github.com/ML-KULeuven/socceraction) using the publicly available [Wyscout match event dataset](https://figshare.com/collections/Soccer_match_event_dataset/4415000). The Wyscout dataset includes data for the 2017/2018 English Premier League, the 2017/2018 Spanish Primera Divisi√≥n, the 2017/2018 German 1. Bundesliga, the 2017/2018 Italian Serie A, the 2017/2018 French Ligue 1, the 2018 FIFA World Cup, and the UEFA Euro 2016. Covering 1,941 matches, 3,251,294 events and 4,299 players, the dataset is large enough to train machine-learning models and obtain robust ratings for the players.\n",
    "\n",
    "This tutorial demonstrates the following four steps:\n",
    "1. Download the [Wyscout dataset](https://figshare.com/collections/Soccer_match_event_dataset/4415000) and preprocess the relevant data.\n",
    "2. Value game states by training predictive machine learning models.\n",
    "  - Compute descriptive features for each game state.\n",
    "  - Obtain labels for each game state (i.e., *Goal scored within next ten actions? Goal conceded within next ten actions?*)\n",
    "3. Value on-the-ball actions by using the trained predictive machine learning models.\n",
    "4. Rate players by aggregating the values of their on-the-ball actions.\n",
    "\n",
    "**Conventions:**\n",
    "* Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "* Variables that refer a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`.\n",
    "\n",
    "**References:**\n",
    "* Tom Decroos, Lotte Bransen, Jan Van Haaren, and Jesse Davis. \"[Actions Speak Louder than Goals: Valuing Player Actions in Soccer.](https://arxiv.org/abs/1802.07127)\" In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, pp. 1851-1861. 2019.\n",
    "* Luca Pappalardo, Paolo Cintia, Alessio Rossi, Emanuele Massucco, Paolo Ferragina, Dino Pedreschi, and Fosca Giannotti. \"[A Public Data Set of Spatio-Temporal Match Events in Soccer Competitions.](https://www.nature.com/articles/s41597-019-0247-7)\" *Scientific Data 6*, no. 1 (2019): 1-15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you run this notebook on Google Colab, then uncomment the code in the following cell and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tables==3.6.1\n",
    "# !pip install socceraction==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you run this notebook on Google Colab and wish to store all data in a Google Drive folder, then uncomment the code in the following cell and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %mkdir -p '/content/gdrive/My Drive/Friends of Tracking/'\n",
    "# %cd '/content/gdrive/My Drive/Friends of Tracking/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "from zipfile import ZipFile, is_zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # version 1.0.3\n",
    "\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score  # version 0.22.2\n",
    "from xgboost import XGBClassifier  # version 1.0.2\n",
    "\n",
    "import socceraction.vaep.features as features\n",
    "import socceraction.vaep.labels as labels\n",
    "\n",
    "from socceraction.spadl.wyscout import convert_to_spadl\n",
    "from socceraction.vaep.formula import value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section downloads the Wyscout dataset, collects the required information about the match events, and converts the match events into the SPADL representation.\n",
    "\n",
    "1. Download the Wyscout dataset;\n",
    "2. Construct an HDF5 file named `wyscout.h5` that contains the relevant information from the dataset;\n",
    "3. Convert the `wyscout.h5` file into a `spadl.h5` file that contains the same information in the SPADL representation.\n",
    "\n",
    "**Note:** The `socceraction` library offers off-the-shelf functionality to convert a collection of Wyscout JSON files into the SPADL representation. However, the JSON files in the publicly available dataset are not directly compatible with the `socceraction` functionality. Therefore, we need to perform a few additional steps to transform the Wyscout data into the SPADL representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Wyscout dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data_files` `dict` lists the four data files in the Wyscout dataset that are required to run the VAEP framework.\n",
    "* `events` (73.74 MB): match events for the matches in the dataset;\n",
    "* `matches` (629.98 kB): overview of the matches in the dataset;\n",
    "* `players` (1.66 MB): information on the players in the dataset;\n",
    "* `teams` (26.76 kB): information on the teams in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    'events': 'https://ndownloader.figshare.com/files/14464685',  # ZIP file containing one JSON file for each competition\n",
    "    'matches': 'https://ndownloader.figshare.com/files/14464622',  # ZIP file containing one JSON file for each competition\n",
    "    'players': 'https://ndownloader.figshare.com/files/15073721',  # JSON file\n",
    "    'teams': 'https://ndownloader.figshare.com/files/15073697'  # JSON file\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loops through the `data_files` `dict`, downloads each listed data file, and stores each downloaded data file to the local file system.\n",
    "\n",
    "If the downloaded data file is a ZIP archive, the included JSON files are extracted from the ZIP archive and stored to the local file system.\n",
    "\n",
    "**Note:** If you do not understand what the code below does exactly, then do not worry too much. ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(data_files.values()):\n",
    "    url_s3 = urlopen(url).geturl()\n",
    "    path = Path(urlparse(url_s3).path)\n",
    "    file_name = path.name\n",
    "    file_local, _ = urlretrieve(url_s3, file_name)\n",
    "    if is_zipfile(file_local):\n",
    "        with ZipFile(file_local) as zip_file:\n",
    "            zip_file.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Wyscout data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_json_file` function reads and returns the content of a given JSON file. The function handles the encoding of special characters (e.g., accents in names of players and teams) that the `pd.read_json` function cannot handle properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    with open(filename, 'rb') as json_file:\n",
    "        return BytesIO(json_file.read()).getvalue().decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the `teams.json` file into a `DataFrame` object and store that object in the `wyscout.h5` HDF5 file under the key `teams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_teams = read_json_file('teams.json')\n",
    "df_teams = pd.read_json(json_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_hdf('wyscout.h5', key='teams', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the `players.json` file into a `DataFrame` object and store that object in the `wyscout.h5` HDF5 file under the key `players`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_players = read_json_file('players.json')\n",
    "df_players = pd.read_json(json_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.to_hdf('wyscout.h5', key='players', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lists the competitions to be included in the dataset. Uncomment the competitions that you want to include in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions = [\n",
    "#     'England',\n",
    "#     'France',\n",
    "#     'Germany',\n",
    "#     'Italy',\n",
    "#     'Spain',\n",
    "    'European Championship',\n",
    "#     'World Cup'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the `matches.json` files for the selected competitions into a `DataFrame` object and store that object in the `wyscout.h5` HDF5 file under the key `matches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_matches = []\n",
    "for competition in competitions:\n",
    "    competition_name = competition.replace(' ', '_')\n",
    "    file_matches = f'matches_{competition_name}.json'\n",
    "    json_matches = read_json_file(file_matches)\n",
    "    df_matches = pd.read_json(json_matches)\n",
    "    dfs_matches.append(df_matches)\n",
    "df_matches = pd.concat(dfs_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.to_hdf('wyscout.h5', key='matches', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the `events.json` files for the selected competitions into a `DataFrame` object and store that object in the `wyscout.h5` HDF5 file under the key `events/match_<match-id>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for competition in competitions:\n",
    "    competition_name = competition.replace(' ', '_')\n",
    "    file_events = f'events_{competition_name}.json'\n",
    "    json_events = read_json_file(file_events)\n",
    "    df_events = pd.read_json(json_events)\n",
    "    df_events_matches = df_events.groupby('matchId', as_index=False)\n",
    "    for match_id, df_events_match in df_events_matches:\n",
    "        df_events_match.to_hdf('wyscout.h5', key=f'events/match_{match_id}', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Wyscout data to the SPADL representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell calls the `convert_to_spadl` function from the `socceraction` library to convert the `wyscout.h5` HDF5 file into the `spadl.h5` HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_spadl('wyscout.h5', 'spadl.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value game states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section generates features and labels for the game states, trains a predictive machine learning model for each label, and values the game states by applying the trained machine learning models.\n",
    "\n",
    "1. Generate the features to describe the game states;\n",
    "2. Generate the labels that capture the value of the game states;\n",
    "3. Compose a dataset by selecting a set of features and the labels of the game states;\n",
    "4. Train predictive machine learning models using the dataset;\n",
    "5. Value the game states using the trained predictive machine learning model.\n",
    "\n",
    "**Note:** The code in this section is based on the [2-compute-features-and-labels.ipynb](https://github.com/ML-KULeuven/socceraction/blob/master/public-notebooks/2-compute-features-and-labels.ipynb) and [3-estimate-scoring-and-conceding-probabilities.ipynb](https://github.com/ML-KULeuven/socceraction/blob/master/public-notebooks/3-estimate-scoring-and-conceding-probabilities.ipynb) notebooks in the `socceraction` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.read_hdf('spadl.h5', key='games')\n",
    "df_actiontypes = pd.read_hdf('spadl.h5', key='actiontypes')\n",
    "df_bodyparts = pd.read_hdf('spadl.h5', key='bodyparts')\n",
    "df_results = pd.read_hdf('spadl.h5', key='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_prev_actions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate game state features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lists a number of *feature generators* from the `features` module in the `socceraction` library. Each function expects either a `DataFrame` object containing actions (i.e., individual actions) or a list of `DataFrame` objects containing consecutive actions (i.e., game states), and returns the corresponding *feature* for the individual action or game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_features = [\n",
    "    features.actiontype_onehot,\n",
    "    features.bodypart_onehot,\n",
    "    features.result_onehot,\n",
    "    features.goalscore,\n",
    "    features.startlocation,\n",
    "    features.endlocation,\n",
    "    features.movement,\n",
    "    features.space_delta,\n",
    "    features.startpolar,\n",
    "    features.endpolar,\n",
    "    features.team,\n",
    "    features.time_delta\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates game states from consecutive actions in each game and computes the features for each game state.\n",
    "\n",
    "1. Obtain the actions for the game (i.e., `df_actions`) by looping through the games;\n",
    "2. Construct game states of a given length from the actions (i.e., `dfs_gamestates`);\n",
    "3. Compute the features for the constructed game states (i.e., `df_features`) by looping through the list of *feature generators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_actions = pd.read_hdf('spadl.h5', key=f'actions/game_{game_id}')\n",
    "    df_actions = (df_actions\n",
    "        .merge(df_actiontypes, how='left')\n",
    "        .merge(df_results, how='left')\n",
    "        .merge(df_bodyparts, how='left')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    dfs_gamestates = features.gamestates(df_actions, nb_prev_actions=nb_prev_actions)\n",
    "    dfs_gamestates = features.play_left_to_right(dfs_gamestates, game['home_team_id'])\n",
    "    \n",
    "    df_features = pd.concat([function(dfs_gamestates) for function in functions_features], axis=1)\n",
    "    df_features.to_hdf('features.h5', key=f'game_{game_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate game state labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lists a number of *label generators* from the `labels` module in the `socceraction` library. Each function expects either a `DataFrame` object containing actions (i.e., individual actions) or a list of `DataFrame` objects containing consecutive actions (i.e., game states), and returns the corresponding *label* for the individual action or game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_labels = [\n",
    "    labels.scores,\n",
    "    labels.concedes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell computes the labels for each action:\n",
    "\n",
    "1. Obtain the actions for the game (i.e., `df_actions`) by looping through the games;\n",
    "2. Compute the labels for the actions (i.e., `df_labels`) by looping through the list of *label generators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_actions = pd.read_hdf('spadl.h5', key=f'actions/game_{game_id}')\n",
    "    df_actions = (df_actions\n",
    "        .merge(df_actiontypes, how='left')\n",
    "        .merge(df_results, how='left')\n",
    "        .merge(df_bodyparts, how='left')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df_labels = pd.concat([function(df_actions) for function in functions_labels], axis=1)\n",
    "    df_labels.to_hdf('labels.h5', key=f'game_{game_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates a list of names for the features to be included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_features = features.feature_column_names(functions_features, nb_prev_actions=nb_prev_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell obtains the relevant features for each game and stores them in the `df_features` `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_features = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_features = pd.read_hdf('features.h5', key=f'game_{game_id}')\n",
    "    dfs_features.append(df_features[columns_features])\n",
    "df_features = pd.concat(dfs_features).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lists the names of the labels to be included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_labels = [\n",
    "    'scores',\n",
    "    'concedes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell obtains the relevant labels for each game and stores them in the `df_labels` `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_labels = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_labels = pd.read_hdf('labels.h5', key=f'game_{game_id}')\n",
    "    dfs_labels.append(df_labels[columns_labels])\n",
    "df_labels = pd.concat(dfs_labels).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains an XGBoost classifier for each label using the computed features. For each label:\n",
    "1. Construct an XGBoost classifier with default hyperparameters;\n",
    "2. Train the classifier using the computed features and the label;\n",
    "3. Store the trained classifier in the `models` `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models = {}\n",
    "for column_labels in columns_labels:\n",
    "    model = XGBClassifier()\n",
    "    model.fit(df_features, df_labels[column_labels])\n",
    "    models[column_labels] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell predicts the labels for the game states using the trained XGBoost classifier. For each label:\n",
    "1. Retrieve the model for the label;\n",
    "2. Estimate the probabilities of the labels being `False` and `True` given the computed features;\n",
    "3. Keep the probabilities for the `True` label;\n",
    "4. Store the probabilities as a `Series` object in the `dfs_predictions` `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_predictions = {}\n",
    "for column_labels in columns_labels:\n",
    "    model = models[column_labels]\n",
    "    probabilities = model.predict_proba(df_features)\n",
    "    predictions = probabilities[:, 1]\n",
    "    dfs_predictions[column_labels] = pd.Series(predictions)\n",
    "df_predictions = pd.concat(dfs_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell obtains the `game_id` for each action in order to store the predictions per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_game_ids = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_actions = pd.read_hdf('spadl.h5', key=f'actions/game_{game_id}')\n",
    "    dfs_game_ids.append(df_actions['game_id'])\n",
    "df_game_ids = pd.concat(dfs_game_ids, axis=0).astype('int').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell concatenates the `DataFrame` objects with predictions and `game_id`s for each action into a single `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.concat([df_predictions, df_game_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell groups the predictions per game based on their `game_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_per_game = df_predictions.groupby('game_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell stores the predictions in the `predictions.h5` HDF5 file per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_id, df_predictions in tqdm(df_predictions_per_game):\n",
    "    df_predictions = df_predictions.reset_index(drop=True)\n",
    "    df_predictions[columns_labels].to_hdf('predictions.h5', key=f'game_{game_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value on-the-ball actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The code in this section is based on the [4-compute-vaep-values.ipynb](https://github.com/ML-KULeuven/socceraction/blob/master/public-notebooks/4-compute-vaep-values.ipynb) notebook in the `socceraction` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = pd.read_hdf('spadl.h5', key='players')\n",
    "df_teams = pd.read_hdf('spadl.h5', key='teams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_values = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_actions = pd.read_hdf('spadl.h5', key=f'actions/game_{game_id}')\n",
    "    df_actions = (df_actions\n",
    "        .merge(df_actiontypes, how='left')\n",
    "        .merge(df_results, how='left')\n",
    "        .merge(df_bodyparts, how='left')\n",
    "        .merge(df_players, how='left')\n",
    "        .merge(df_teams, how='left')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df_predictions = pd.read_hdf('predictions.h5', key=f'game_{game_id}')\n",
    "    df_values = value(df_actions, df_predictions['scores'], df_predictions['concedes'])\n",
    "    \n",
    "    df_all = pd.concat([df_actions, df_predictions, df_values], axis=1)\n",
    "    dfs_values.append(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = (pd.concat(dfs_values)\n",
    "    .sort_values(['game_id', 'period_id', 'time_seconds'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values[\n",
    "    ['short_name', 'scores', 'concedes', 'offensive_value', 'defensive_value', 'vaep_value']\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The code in this section is based on the [5-top-players.ipynb](https://github.com/ML-KULeuven/socceraction/blob/master/public-notebooks/5-top-players.ipynb) notebook in the `socceraction` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate according to total VAEP value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking = (df_values[['player_id', 'team_name', 'short_name', 'vaep_value']]\n",
    "    .groupby(['player_id', 'team_name', 'short_name'])\n",
    "    .agg(vaep_count=('vaep_value', 'count'), vaep_sum=('vaep_value', 'sum'))\n",
    "    .sort_values('vaep_sum', ascending=False)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate according to total VAEP value per 90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_games = pd.read_hdf('spadl.h5', 'player_games')\n",
    "df_player_games = df_player_games[df_player_games['game_id'].isin(df_games['game_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes_played = (df_player_games[['player_id', 'minutes_played']]\n",
    "    .groupby('player_id')\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes_played.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90 = df_ranking.merge(df_minutes_played)\n",
    "df_ranking_p90 = df_ranking_p90[df_ranking_p90['minutes_played'] > 360]\n",
    "df_ranking_p90['vaep_rating'] = df_ranking_p90['vaep_sum'] * 90 / df_ranking_p90['minutes_played']\n",
    "df_ranking_p90 = df_ranking_p90.sort_values('vaep_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90.to_csv('ranking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "498.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
