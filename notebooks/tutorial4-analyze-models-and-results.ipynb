{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Analyze models and results\n",
    "This tutorial demonstrates how to analyze the trained machine learning models to value on-the-ball actions of football players with the open-source [VAEP framework](https://github.com/ML-KULeuven/socceraction) using the publicly available [Wyscout match event dataset](https://figshare.com/collections/Soccer_match_event_dataset/4415000). The Wyscout dataset includes data for the 2017/2018 English Premier League, the 2017/2018 Spanish Primera Divisi√≥n, the 2017/2018 German 1. Bundesliga, the 2017/2018 Italian Serie A, the 2017/2018 French Ligue 1, the 2018 FIFA World Cup, and the UEFA Euro 2016. Covering 1,941 matches, 3,251,294 events and 4,299 players, the dataset is large enough to train machine-learning models and obtain robust ratings for the players.\n",
    "\n",
    "This tutorial demonstrates the following three steps:\n",
    "   1. Analyze feature importances of the model.\n",
    "   2. Analyze single predictions for game states.\n",
    "   3. Analyze the player ratings that can be calculated using the two models trained in tutorial 3.\n",
    "\n",
    "**Conventions:**\n",
    "   * Variables that refer a `DataFrame` object are prefixed with `df_`.\n",
    "   * Variables that refer a collection of `DataFrame` objects (e.g., a list, a set or a dict) are prefixed with `dfs_`.\n",
    "   \n",
    "**References:**\n",
    "   * Tom Decroos, Lotte Bransen, Jan Van Haaren, and Jesse Davis. \\\"[Actions Speak Louder than Goals: Valuing Player Actions in Soccer.](https://arxiv.org/abs/1802.07127)\\\" In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, pp. 1851-1861. 2019.\n",
    "   * Luca Pappalardo, Paolo Cintia, Alessio Rossi, Emanuele Massucco, Paolo Ferragina, Dino Pedreschi, and Fosca Giannotti. \\\"[A Public Data Set of Spatio-Temporal Match Events in Soccer Competitions.](https://www.nature.com/articles/s41597-019-0247-7)\\\" *Scientific Data 6*, no. 1 (2019): 1-15.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you run this notebook on Google Colab, then uncomment the code in the following cell and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tables==3.6.1\n",
    "# !pip install socceraction==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you run this notebook on Google Colab and wish to store all data in a Google Drive folder, then uncomment the code in the following cell and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %mkdir -p '/content/gdrive/My Drive/Friends of Tracking/'\n",
    "# %cd '/content/gdrive/My Drive/Friends of Tracking/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd  # version 1.0.3\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance  # version 1.0.2\n",
    "\n",
    "import shap  # version 0.35.0\n",
    "\n",
    "from ipywidgets import interact_manual, fixed, widgets  # version 7.5.1\n",
    "\n",
    "from socceraction.vaep.formula import value # version 0.2.0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fourth tutorial assumes that the spadl.h5 HDF5 file as well as the features.h5, labels.h5 and predictions.h5 files have been created for a set of games in the first or second tutorial.\n",
    "\n",
    "This fourth tutorial only uses features that have been generated in the first tutorial. However, you are strongly encouraged to toy around with the additional features from the second tutorial and to try out your own features to improve the accuracy of the predictive machine learning models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.read_hdf('spadl.h5', key='games')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the trained models to rate actions for all actions from the 2017/2018 Spanish La Liga (competition_id = 795). Therefore, we make sure that we don't use this data when training and tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_train = df_games[\n",
    "    df_games['competition_id'] != 795\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_train.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_test = df_games[\n",
    "    df_games['competition_id'] == 795\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features for the selected games and combine them into the df_features DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_features = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']\n",
    "    df_features = pd.read_hdf('features.h5', key=f'game_{game_id}')\n",
    "    df_features['game_id'] = game_id\n",
    "    dfs_features.append(df_features)\n",
    "df_features = pd.concat(dfs_features).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the labels for the selected games and combine them into the df_labels DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_labels = []\n",
    "for _, game in tqdm(df_games.iterrows(), total=len(df_games)):\n",
    "    game_id = game['game_id']  \n",
    "    df_labels = pd.read_hdf('labels.h5', key=f'game_{game_id}')\n",
    "    df_labels['game_id'] = game_id\n",
    "    dfs_labels.append(df_labels)\n",
    "df_labels = pd.concat(dfs_labels).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fourth tutorial assumes that you followed the third tutorial. In that tutorial it is explained in detail how you can split the data and train your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_features[df_features['game_id'].isin(df_games_train['game_id'].unique())]\n",
    "df_X_test = df_features[df_features['game_id'].isin(df_games_test['game_id'].unique())]\n",
    "df_y_train = df_labels[df_labels['game_id'].isin(df_games_train['game_id'].unique())]\n",
    "df_y_test = df_labels[df_labels['game_id'].isin(df_games_test['game_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides a list of features that the machine learning algorithm will consider to train the model. The selected features exist in the features.h5 file that was created in the second tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'start_distance_to_goal-0',\n",
    "    'end_distance_to_goal-0',\n",
    "    'start_distance_to_goal-1',\n",
    "    'end_distance_to_goal-1',\n",
    "    'start_distance_to_goal-2',\n",
    "    'end_distance_to_goal-2',\n",
    "    'start_angle_to_goal-0',\n",
    "    'end_angle_to_goal-0',\n",
    "    'start_angle_to_goal-1',\n",
    "    'end_angle_to_goal-1',\n",
    "    'start_angle_to_goal-2',\n",
    "    'end_angle_to_goal-2',\n",
    "    'team-1',\n",
    "    'team-2',\n",
    "    'result_id-0',\n",
    "    'result_id-1',\n",
    "    'result_id-2'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides a list of class labels for which the machine learning algorithm will train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The concedes class label has been commented to speed up the execution of the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'scores',\n",
    "    'concedes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains an XGBoost classifier for each label using conservative hyperparamters for the learning algorithm, which will serve as baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for label in tqdm(labels):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3\n",
    "    )\n",
    "    model.fit(\n",
    "        X=df_X_train[features],\n",
    "        y=df_y_train[label]\n",
    "    )\n",
    "    models[label] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    plot_importance(models[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='scores'\n",
    "#label='concedes'\n",
    "\n",
    "explainer = shap.TreeExplainer(models[label])\n",
    "\n",
    "shap_values = explainer.shap_values(df_X_test[features])\n",
    "shap.summary_plot(shap_values, df_X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell estimates the probabilities for the game states in the test set for each label using the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_predictions = {}\n",
    "for label in tqdm(labels):\n",
    "    model = models[label]\n",
    "    probabilities = model.predict_proba(\n",
    "        df_X_test[features]\n",
    "    )\n",
    "    predictions = probabilities[:, 1]\n",
    "    dfs_predictions[label] = pd.Series(predictions, index=df_X_test.index)\n",
    "df_predictions = pd.concat(dfs_predictions, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load actions, players and teams and value actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = pd.read_hdf('spadl.h5', key='players')\n",
    "df_teams = pd.read_hdf('spadl.h5', key='teams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_actions = []\n",
    "for _, game in tqdm(df_games_test.iterrows(), total=len(df_games_test)):\n",
    "    game_id = game['game_id']\n",
    "    with pd.HDFStore('spadl.h5') as spadlstore:\n",
    "        df_actions = spadlstore[f'actions/game_{game_id}']\n",
    "        df_actions = (\n",
    "            df_actions.merge(spadlstore['actiontypes'], how='left')\n",
    "            .merge(spadlstore['results'], how='left')\n",
    "            .merge(spadlstore['bodyparts'], how='left')\n",
    "            .merge(spadlstore['players'], how='left')\n",
    "            .merge(spadlstore['teams'], how='left')\n",
    "            .reset_index()\n",
    "            .rename(columns={'index': 'action_id'})\n",
    "        )\n",
    "    \n",
    "    dfs_actions.append(df_actions)\n",
    "df_actions = pd.concat(dfs_actions).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_predictions = pd.concat([df_actions, df_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_values = []\n",
    "for game_id, game_predictions in df_actions_predictions.groupby('game_id'):\n",
    "    df_values = value(game_predictions, game_predictions['scores'], game_predictions['concedes'])\n",
    "    \n",
    "    df_all = pd.concat([game_predictions, df_values], axis=1)\n",
    "    dfs_values.append(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = (pd.concat(dfs_values)\n",
    "    .sort_values(['game_id', 'period_id', 'time_seconds'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the ratings per player and look at the quantity-quality trade-off, risk-reward trade-off and the rating per player per action type (e.g. shot, pass, tackle, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking = (df_values[['player_id', 'team_name', 'short_name', 'vaep_value']]\n",
    "    .groupby(['player_id', 'team_name', 'short_name'])\n",
    "    .agg(vaep_count=('vaep_value', 'count'), \n",
    "         vaep_mean=('vaep_value', 'mean'),\n",
    "         vaep_sum=('vaep_value', 'sum'))\n",
    "    .sort_values('vaep_sum', ascending=False)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_games = pd.read_hdf('spadl.h5', 'player_games')\n",
    "df_player_games = df_player_games[df_player_games['game_id'].isin(df_games['game_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes_played = (df_player_games[['player_id', 'minutes_played']]\n",
    "    .groupby('player_id')\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minutes_played.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90 = df_ranking.merge(df_minutes_played)\n",
    "df_ranking_p90['vaep_rating'] = df_ranking_p90['vaep_sum'] * 90 / df_ranking_p90['minutes_played']\n",
    "df_ranking_p90['actions_p90'] = df_ranking_p90['vaep_count'] * 90 / df_ranking_p90['minutes_played']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_THRESHOLD = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90 = df_ranking_p90[df_ranking_p90['minutes_played']>MIN_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranking_p90 = df_ranking_p90.sort_values('vaep_rating', ascending=False)\n",
    "df_ranking_p90.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantity - quality trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we create a figure to analyze the quantity-quality trade-off. The VAEP rating per player per 90 minutes depends on the average rating per action and the number of actions per 90 minutes. Some players have high ratings because they execute a lot of actions per 90 minutes, while others have very high average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "x = list(df_ranking_p90['vaep_mean'])\n",
    "y = list(df_ranking_p90['actions_p90'])\n",
    "plt.plot(x, y, '.', c='#1C3460', markersize=15)\n",
    "\n",
    "# Plot 5 best players\n",
    "x_best = list(df_ranking_p90['vaep_mean'][0:5])\n",
    "y_best = list(df_ranking_p90['actions_p90'][0:5])\n",
    "names = list(df_ranking_p90['short_name'][0:5])\n",
    "names = [name.split(\".\")[-1] for name in names]\n",
    "plt.plot(x_best, y_best, '.', c='#D62A2E', markersize=15)\n",
    "for i, txt in enumerate(names):\n",
    "    plt.annotate(txt, (x[i], y[i] + 2), fontsize=20, horizontalalignment='center')\n",
    "\n",
    "best_player = x[0] * y[0]\n",
    "yi = np.arange(0.1, 100, 0.1)\n",
    "xi = [best_player / i for i in yi]\n",
    "plt.plot(xi, yi, '--', c='grey')\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlim(0, 0.03)\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel('Average VAEP rating per action', labelpad=20, fontsize=20)\n",
    "plt.ylabel('Total\\nnumber of\\nactions per\\n90 minutes', rotation=0, labelpad=20,\n",
    "            verticalalignment='center', horizontalalignment='right', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk - reward trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we will look at the ratings for a player's successful actions and for the player's unsuccessful actions. In this way we can identify players that take more risk, and players that play less risky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = pd.pivot_table(df_values, values='vaep_value', index=['player_id', 'short_name', 'team_name'],\n",
    "                                              columns=['result_name'], aggfunc=np.sum,\n",
    "                                              fill_value=0)[['success', 'fail']]\n",
    "df_risk = df_risk.reset_index()\n",
    "df_risk = df_risk[0:-1]\n",
    "df_risk = pd.merge(df_risk, df_minutes_played, on=['player_id'])\n",
    "for result in ['success', 'fail']:\n",
    "    df_risk[result] = df_risk[result] / df_risk['minutes_played'] * 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = df_risk[df_risk['minutes_played']>MIN_THRESHOLD]\n",
    "df_risk['vaep_rating'] = df_risk['success'] + df_risk['fail']\n",
    "df_risk.sort_values('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = df_risk.sort_values('vaep_rating', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = list(df_risk['fail'])\n",
    "y = list(df_risk['success'])\n",
    "plt.plot(x, y, '.', c='#1C3460', markersize=15)\n",
    "\n",
    "# Plot 5 best players\n",
    "x_best = list(df_risk['fail'][0:5])\n",
    "y_best = list(df_risk['success'][0:5])\n",
    "names = list(df_risk['short_name'][0:5])\n",
    "names = [name.split(\".\")[-1] for name in names]\n",
    "plt.plot(x_best, y_best, '.', c='#D62A2E', markersize=15)\n",
    "for i, txt in enumerate(names):\n",
    "    plt.annotate(txt, (x[i], y[i] + 0.01), fontsize=20, horizontalalignment='center')\n",
    "\n",
    "best_player = x[0] + y[0]\n",
    "yi = np.arange(0.1, 2, 0.1)\n",
    "xi = [best_player - i for i in yi]\n",
    "plt.plot(xi, yi, '--', c='grey')\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlim(-0.25, 0.01)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.xlabel('Total VAEP rating with unsuccessful actions', rotation=0, labelpad=20,\n",
    "            verticalalignment='center', horizontalalignment='right', fontsize=20)\n",
    "plt.ylabel('Total\\nVAEP rating\\nwith successful\\nactions', labelpad=20, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating per action type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we will analyze the rating per player per action type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_action = pd.pivot_table(df_values, values='vaep_value', index=['player_id', 'short_name', 'team_name'],\n",
    "                                              columns=['type_name'], aggfunc=np.sum,\n",
    "                                              fill_value=0, margins=True, margins_name='total')\n",
    "df_rating_action = df_rating_action.reset_index()\n",
    "df_rating_action = df_rating_action[0:-1]\n",
    "df_rating_action = pd.merge(df_rating_action, df_minutes_played, on=['player_id'])\n",
    "for action in df_values['type_name'].unique():\n",
    "    df_rating_action[action] = df_rating_action[action] / df_rating_action['minutes_played'] * 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_action[df_rating_action['minutes_played']>MIN_THRESHOLD].sort_values('shot', \n",
    "                                                                               ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:friends-of-tracking]",
   "language": "python",
   "name": "conda-env-friends-of-tracking-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
